{"cells":[{"cell_type":"markdown","metadata":{"id":"o_O2PoR1j0jc"},"source":["# Análisis de sentimientos en Twitter con Transfer Learning\n","\n","El objetivo de este notebook es utilizar Transfer Learning sobre un conjunto de datos para obtener el sentimiento en los tweets publicados por los usuarios de la plataforma."]},{"cell_type":"markdown","metadata":{"id":"OoBhReI_lSre"},"source":["## 1. Librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19823,"status":"ok","timestamp":1648584335131,"user":{"displayName":"Sergio M","userId":"05689582408971891814"},"user_tz":-120},"id":"Sz-mIkWrJgah","outputId":"e8a02ba3-b9ab-49c2-c88d-94e77a0623f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: texthero in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n","Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.63.0)\n","Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.7)\n","Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.4)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n","Requirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (3.10.0.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2022.3.15)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.8.1)\n","Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.44.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.24.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n","Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0.dev2021122109)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.14.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.5.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.5)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n"]}],"source":["!pip install texthero\n","!pip install tensorflow-text"]},{"cell_type":"code","source":["import tensorflow_text as tf_text\n"],"metadata":{"id":"n8YC7Un7SFy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_text.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"mAuFWGe7SG0j","executionInfo":{"status":"ok","timestamp":1648584337934,"user_tz":-120,"elapsed":3,"user":{"displayName":"Sergio M","userId":"05689582408971891814"}},"outputId":"3eb2ce32-d413-46d3-b5d7-228fef0459b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1128922,"status":"ok","timestamp":1648550014716,"user":{"displayName":"Sergio M","userId":"05689582408971891814"},"user_tz":-120},"id":"VdKqfTC1j7Pu","outputId":"a51295f1-6fe3-4ff0-cf57-7e4f4c22909f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Procesamiento de datos\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Visualización\n","import matplotlib.pyplot as plt\n","import seaborn \n","\n","# NLP\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer \n","from collections import Counter\n","from nltk.tokenize import RegexpTokenizer\n","import re\n","import string\n","\n","# TensorFlow\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow import keras as ks\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing import sequence\n","import tensorflow_text as tf_text\n","\n","%load_ext tensorboard\n","\n","#Library for Text Processing\n","import texthero as hero\n","\n","#Sk Learn Library\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","#Tabulate\n","from tabulate import tabulate\n","\n","# General\n","import sys\n","import os\n","import gc\n","import time\n","import datetime\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Definimos la ruta donde teneis los archivos NPZ dentro de Google Drive\n","filename_path = \"/content/drive/MyDrive/Colab Notebooks/TweetSentimentAnalysis/data/training.1600000.processed.noemoticon.csv\"\n"]},{"cell_type":"markdown","metadata":{"id":"rYxVu92-j6KG"},"source":["## 2. Cargar datos\n","\n","El dataset utilizado es [__sentiment140__](http://help.sentiment140.com/for-students) y consta de 1.600.000 tweets extraídos con la API de Twitter que puede ser utilizado para detectar sentimientos. Consta de 6 variables que se detallan a continuación:\n","\n","- __target__. Polaridad del tweet (0 = negativo, 2 = neutral, 4 = positivo)\n","\n","- __ids__. El id del tweet.\n","\n","- __date__. Fecha del tweet (sábado 16 de mayo 23:58:44 UTC 2009)\n","\n","- __flag__. Consultai ( lyx ). Si no hay consulta, entonces este valor es NO_QUERY.\n","\n","- __usuario__. Ul usuario que tuiteó.\n","\n","- __text__. Texto del tweet.\n","\n","\n","Las variables que se utilizarán son:\n","- target.\n","- text."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"elapsed":13097,"status":"ok","timestamp":1648321061056,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"zbUo4_hUjYoo","outputId":"493e13ce-0f89-4970-83bf-e9c5141ba17f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cargando dataset...\n","Dimensionalidad: (1600000, 6)\n"]},{"data":{"text/html":["\n","  <div id=\"df-1714cbb5-3674-448d-9c49-651d5d0189b2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>ids</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810369</td>\n","      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>_TheSpecialOne_</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1714cbb5-3674-448d-9c49-651d5d0189b2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1714cbb5-3674-448d-9c49-651d5d0189b2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1714cbb5-3674-448d-9c49-651d5d0189b2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   target         ids                          date      flag  \\\n","0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n","1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n","2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n","3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","\n","              user                                               text  \n","0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n","1    scotthamilton  is upset that he can't update his Facebook by ...  \n","2         mattycus  @Kenichan I dived many times for the ball. Man...  \n","3          ElleCTF    my whole body feels itchy and like its on fire   \n","4           Karoli  @nationwideclass no, it's not behaving at all....  "]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["print('Cargando dataset...')\n","df = pd.read_csv(filename_path, encoding='latin', header = None)\n","df.columns = ['target', 'ids', 'date','flag','user','text']\n","df.sort_values('date', ascending=True, inplace=True)\n","print('Dimensionalidad:',df.shape)\n","df.head()"]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"VAlPnIbhSBXE","executionInfo":{"status":"error","timestamp":1648584313482,"user_tz":-120,"elapsed":2889,"user":{"displayName":"Sergio M","userId":"05689582408971891814"}},"outputId":"371d60e7-2644-4755-d69c-7a0c0415c361"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7af2306084ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Various tensorflow ops related to text-processing.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_undocumented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpHint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_toco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_computation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_xla_computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0m_xla_computation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtanh\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from jax._src.nn.functions import (\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mcelu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0melu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/nn/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxisName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_logsumexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/scipy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/scipy/signal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# flake8: noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from jax._src.scipy.signal import (\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mconvolve\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconvolve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mconvolve2d\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconvolve2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/scipy/signal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mosp_signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/signal/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_peak_finding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwindows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_window\u001b[0m  \u001b[0;31m# keep this one in signal namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/signal/_peak_finding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mricker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscoreatpercentile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m from ._peak_finding_utils import (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                     rv_frozen)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# to approximate the pdf of a continuous distribution given its cdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m from numpy import (arange, putmask, ravel, ones, shape, ndarray, zeros, floor,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/misc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'doccer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mrelease\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"9aalh4A3SDJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1648321280908,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"FJYvSoYV4kTp","outputId":"35b12242-3d2d-4f23-9c27-94e2e20c2b82"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-6505b3d9-c9cf-46bf-bf3c-4c49146e6a38\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>ids</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1467810369</th>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467810672</th>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467810917</th>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467811184</th>\n","      <td>my whole body feels itchy and like its on fire</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467811193</th>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6505b3d9-c9cf-46bf-bf3c-4c49146e6a38')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6505b3d9-c9cf-46bf-bf3c-4c49146e6a38 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6505b3d9-c9cf-46bf-bf3c-4c49146e6a38');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                         text  target\n","ids                                                                  \n","1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...       0\n","1467810672  is upset that he can't update his Facebook by ...       0\n","1467810917  @Kenichan I dived many times for the ball. Man...       0\n","1467811184    my whole body feels itchy and like its on fire        0\n","1467811193  @nationwideclass no, it's not behaving at all....       0"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["# Seleccionar columnas de interés\n","df = df[['ids','text', 'target']]\n","df.set_index('ids', inplace=True)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"HPjQzZihld5u"},"source":["El identificador del tweet es una clave primaria, es decir, no pueden existir duplicados y en el caso de existir son tweets duplicados. Por este motivo, cuantificamos el número de duplicados y los eliminamos manteniendo el primero."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1685,"status":"ok","timestamp":1648321298339,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"tD8hCN7RldNX","outputId":"71e20e77-1c54-4f49-c7a2-835b6db65ab5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensionalidad: (1583691, 2)\n"]}],"source":["df.drop_duplicates(keep='first', inplace=True)\n","print('Dimensionalidad:',df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1648321302655,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"3vbjITiOmwEL","outputId":"91ca8a9d-ecbe-4377-c0e1-e8c3615d0535"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-9649816b-1bba-4e8f-84a4-9cdb8981dafe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>ids</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1467810369</th>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467810672</th>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467810917</th>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467811184</th>\n","      <td>my whole body feels itchy and like its on fire</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1467811193</th>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9649816b-1bba-4e8f-84a4-9cdb8981dafe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9649816b-1bba-4e8f-84a4-9cdb8981dafe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9649816b-1bba-4e8f-84a4-9cdb8981dafe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                         text  target\n","ids                                                                  \n","1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...       0\n","1467810672  is upset that he can't update his Facebook by ...       0\n","1467810917  @Kenichan I dived many times for the ball. Man...       0\n","1467811184    my whole body feels itchy and like its on fire        0\n","1467811193  @nationwideclass no, it's not behaving at all....       0"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65072,"status":"ok","timestamp":1648321615434,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"4CCVSiCr4zTR","outputId":"6f8473cf-910b-4929-fdc6-11457fbc4889"},"outputs":[{"name":"stdout","output_type":"stream","text":["Procesando texto...\n","CPU times: user 54.4 s, sys: 489 ms, total: 54.9 s\n","Wall time: 1min 4s\n"]}],"source":["%%time\n","# Crear nueva columna con texto procesado\n","print('Procesando texto...')\n","df['text_clean'] = df['text'].pipe(hero.clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1648321616214,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"Qgww0CX8nt-W","outputId":"52187ccf-0401-451a-d1ec-2f3dec5a7fac"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-79c48df0-1ae4-4d0a-af14-e7327772eb09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","    <tr>\n","      <th>ids</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1467810369</th>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","      <td>0</td>\n","      <td>switchfoot http twitpic com 2y1zl awww bummer ...</td>\n","    </tr>\n","    <tr>\n","      <th>1467810672</th>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","      <td>0</td>\n","      <td>upset update facebook texting might cry result...</td>\n","    </tr>\n","    <tr>\n","      <th>1467810917</th>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","      <td>0</td>\n","      <td>kenichan dived many times ball managed save re...</td>\n","    </tr>\n","    <tr>\n","      <th>1467811184</th>\n","      <td>my whole body feels itchy and like its on fire</td>\n","      <td>0</td>\n","      <td>whole body feels itchy like fire</td>\n","    </tr>\n","    <tr>\n","      <th>1467811193</th>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","      <td>0</td>\n","      <td>nationwideclass behaving mad see</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79c48df0-1ae4-4d0a-af14-e7327772eb09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-79c48df0-1ae4-4d0a-af14-e7327772eb09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-79c48df0-1ae4-4d0a-af14-e7327772eb09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                         text  target  \\\n","ids                                                                     \n","1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...       0   \n","1467810672  is upset that he can't update his Facebook by ...       0   \n","1467810917  @Kenichan I dived many times for the ball. Man...       0   \n","1467811184    my whole body feels itchy and like its on fire        0   \n","1467811193  @nationwideclass no, it's not behaving at all....       0   \n","\n","                                                   text_clean  \n","ids                                                            \n","1467810369  switchfoot http twitpic com 2y1zl awww bummer ...  \n","1467810672  upset update facebook texting might cry result...  \n","1467810917  kenichan dived many times ball managed save re...  \n","1467811184                   whole body feels itchy like fire  \n","1467811193                   nationwideclass behaving mad see  "]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1648321616579,"user":{"displayName":"Sergio M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnvFcKX_ez9yqO91FNVWH9qXpRhbFWw7BL_SjDLg=s64","userId":"05689582408971891814"},"user_tz":-60},"id":"clqbGaIV69PV","outputId":"c6449bd2-b064-4a00-b8b1-23da525926d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 89.9 ms, sys: 1.87 ms, total: 91.7 ms\n","Wall time: 137 ms\n"]}],"source":["%%time\n","#Encode the Label to convert it into numerical values [Fake = 0; Real = 1]\n","lab_enc = LabelEncoder()\n","\n","#Applying to the dataset\n","df['target'] = lab_enc.fit_transform(df['target'])"]},{"cell_type":"markdown","metadata":{"id":"U5WiKBQBlO7p"},"source":["## 3. Modelado\n"]},{"cell_type":"markdown","metadata":{"id":"SHbcZ3BRhjgE"},"source":["El modelo consiste en determinar si el sentimiento de un tweet es positivo o negativo mediante aprendizaje profundo.\n","\n","Se utilizará como primera capa redes pre-entrenadas de incrustación de texto.de la red neuronal. Las ventajas de utilizar esta práctica son:\n","- No es necesario procesar el texto.\n","- Se utiliza el transfer learning de modelos entrenados con una gran cantidad de datos que generalmente funcionan muy bien.\n","- Es fácil de procesar porque su tamaño es fijo.\n","\n","Los modelos que se utilizarán son:\n","- __TensorFlow__. [tf2-preview/gnews-swivel-20dim](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1). Incrustación de texto basada en token entrenada en el corpus de 130 GB de Google News en inglés.\n","\n","- __Google__. [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1). Es idéntico a google/tf2-preview/gnews-swivel-20dim/1, pero con un 2,5% de vocabulario convertido en buckets OOV. Esta opción es útil cuando el vocabulario de la aplicación y el vocabulario del modelo no coinciden totalmente.\n","- __Google__. [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1).Modelo muy superior con un vocabulario de ~1M y 50 dimensiones.\n","- __Google__. [google/tf2-preview/nnlm-en-dim128](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1). Modelo aun mayor con un vocabulario de ~1M y 128 dimensiones.\n","\n","\n","Se crearán dos modelo por cada uno de los listados anteriormente que utilizan el texto original y el procesado. Por lo tanto, cada tweet tendrá un total de 8 probabilidades con valor en el intervalo 0,1 que indicará el grado de sentimiento positivo de un determinado tweet.\n","\n","Las capas se apilan secuencialmente para construir el clasificador:\n","\n","- La primera capa es una capa TensorFlow Hub. Esta capa tiene el modelo pre-entrenado para mapear una frase en su vector de incrustación. El modelo de incrustación de texto preentrenado divide la frase en tokens, incrusta cada token y luego combina la incrustación. Las dimensiones resultantes son: (num_samples, embedding_dimension). \n","\n","- El vector de salida de longitud fija se canaliza a través de una capa totalmente conectada (densa) con 16 unidades ocultas.\n","\n","- La última capa está densamente conectada con un único nodo de salida.\n","\n","\n","Por otro lado, el modelo necesita definir una función de pérdida y un optimizador para el entrenamiento. Dado que el conjunto de datos tiene una clasificación binaria y el modelo produce logits (una capa de una sola unidad con una activación lineal), se utilizará la función de pérdida binary_crossentropy."]},{"cell_type":"markdown","metadata":{"id":"c4XKfJnZ6fBl"},"source":["### 3.2. Modelado"]},{"cell_type":"markdown","metadata":{"id":"eJxuQGo_n9Zw"},"source":["A continuación, se crea el objeto __Fayspy_Twitter__ que dispone de los métodos y atributos que permiten crear el modelo de aprendizaje profundo, entrenarlo y almacenarlo. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3OGu_AHo5f9"},"outputs":[],"source":["class Fayspy_Twitter():\n","  \"\"\"\n","  Objeto diseñado por Fayspy para clasificar el sentimiento de los tweets en dos\n","  positivo (1) o negativo (0) mediante Transfer Learning \n","  \"\"\"\n","  # Atributos de clase\n","  models = dict() # Diccionario de modelos utilizados por el objeto.\n","\n","  def __init__(self):\n","    \"\"\"\n","    Constructor.\n","    \"\"\"    \n","    print('FaysPy Twitter Natural Language Processing created!')\n","\n","  def __validate_df(self, df):\n","    \"\"\"\n","    Validar que el dataset cumple con las restricciones necesarias.\n","      - Las columnas tener por nombre ids, text y sentiment.\n","      - La variable objetivo debe ser binaria (0,1)\n","\n","    Args:\n","    -----\n","      df [pandas.DataFrame] -- Dataset a validar\n","\n","    Returns:\n","      validated [bool] -- True si el dataset cumple las restricciones y 0 si no\n","                          las cumple.\n","    \"\"\"\n","    vars = sorted(['ids', 'text','target'])\n","\n","    # 1. Comprobar que las columnas son 'text' y 'sentiment'.\n","    if not sorted([x for x in vars if x in df.columns]) == vars:\n","      raise('Error! El dataset debe contener las columnas text y target')\n","      return Fasle\n","\n","    # 2. Comprobar que la variable target tiene dos posibles valores\n","    if not df.target.unique().shape[0] == 2:\n","      raise('Error! La variable objetivo debe ser binaria (sentimiento positivo = 0; sentimiento negativo = 1)')\n","      return False\n","\n","    return True\n","\n","  def prepare_dataset(self, data_df):\n","    \"\"\"\n","    Método que permite pre-procesar el dataset original de tweets utilizado\n","    en el entrenamiento de los modelos de aprendizaje.\n","\n","    Args:\n","    ------\n","      data_df [pandas.DataFrame] -- Conjunto de entrenamiento. Debe contener las \n","                                    columnas 'text' y 'sentiment' y debe estar \n","                                    indexado con el id de los tweets.\n","\n","    Returns:\n","    ------\n","      nlp_df [pandas.DataFrame] -- Dataset preparado para ser utilizado por el método\n","                                   train_val_split().\n","    \"\"\"\n","    if self.__validate_df(data_df):\n","      nlp_df = data_df.copy()#sort_values('date', ascending=True)\n","      print('Dimensionalidad:',data_df.shape)\n","\n","      # Seleccionar columnas de interés\n","      print('Selecting vars')\n","      nlp_df = nlp_df[['ids','text', 'target']]\n","      nlp_df.set_index('ids', inplace=True)\n","\n","      # Elimnar duplicados\n","      print('Drop duplicates...')\n","      nlp_df.drop_duplicates(keep='first', inplace=True)\n","      print('Dimensionalidad:', nlp_df.shape)\n","\n","      # Pre-procesamiento del texto\n","      def clean_ascii(text):\n","          # function to remove non-ASCII chars from data\n","          return ''.join(i for i in text if ord(i) < 128)\n","      print('Cleaning ascii...')\n","      nlp_df['text'] = nlp_df['text'].apply(clean_ascii)\n","\n","      # Convertir palabras a minúsculas\n","      print('Lower text...')\n","      nlp_df['text'] = nlp_df['text'].str.lower()\n","\n","      # Limpieza y eliminar stop words en inglés\n","      print('Cleaning stopwords...')\n","      stopwords_list = list(stopwords.words('english'))\n","      def cleaning_stopwords(text):\n","        return ' '.join([word for word in str(text).split() if word not in stopwords_list])\n","\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_stopwords(x))\n","\n","\n","      # Limpieza y eliminar signos de puntuación\n","      print('Cleaning english punctuations...')\n","      english_punctuations = string.punctuation\n","      def cleaning_punctuations(text):\n","        translator = str.maketrans('','',english_punctuations)\n","        return text.translate(translator)\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_punctuations(x))\n","\n","      # Limpieza y eliminar caracteres repetidos\n","      print('Cleaning repeating chars...')\n","      def cleaning_repeating_char(text):\n","        return re.sub(r'(.)\\1+',r'\\1', text)\n","\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_repeating_char(x))\n","\n","      # Limpieza y eliminar emails\n","      print('Cleaning emails...')\n","      def cleaning_email(data):\n","        return re.sub('@[^\\s]+', ' ', data)\n","\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_email(x))\n","\n","      # Limpieza y eliminar URL's\n","      print('Cleaning urls...')\n","      def cleaning_URL(data):\n","        return re.sub('((www\\.[^\\s]+)(https:?://[^\\s]+))', ' ', data)\n","\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_URL(x))\n","\n","      # Limpieza y eliminar numeros\n","      print('Cleaning numbers...')\n","      def cleaning_numbers(data):\n","        return re.sub('[0-9]+',' ',data)\n","\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: cleaning_numbers(x))\n","\n","      # Tokenización del texto de los tweets\n","      print('Tokenizing text...')\n","      tokenizer = RegexpTokenizer(r'\\w+')\n","      nlp_df['text'] = nlp_df['text'].apply(tokenizer.tokenize)\n","      \n","      # Aplicar Stemming\n","      print('Applying stemming...')\n","      st = nltk.PorterStemmer()\n","      def stemming_on_text(data):\n","        text = [st.stem(word) for word in data]\n","        return data\n","      \n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: stemming_on_text(x))\n","      \n","\n","      # Aplicar Lemmatizer\n","      print('Applying lemmatizer...')\n","      nltk.download('wordnet')\n","      nltk.download('omw-1.4')\n","      lm = nltk.WordNetLemmatizer()\n","      \n","      def lemmatizer_on_text(data):\n","        text = [lm.lemmatize(word) for word in data]\n","        return text\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: lemmatizer_on_text(x))\n","      \n","      # Conversión de vector a lista para la red neuronal\n","      nlp_df['text'] = nlp_df['text'].apply(lambda x: ' '.join(x))\n","\n","      return nlp_df\n","\n","\n","  def train_test_split(self, nlp_df, test_size=.2, val_size=.1, feature='text'):\n","    \"\"\"\n","    Método que divide el dataset en conjunto de entrenamiento y validación.\n","\n","    Args:\n","    -----\n","        nlp_df [pandas.DataFrame] -- Conjunto de datos resultado del método\n","                                      prepare_dataset()\n","\n","        test_size [float] -- Tamaño del conjunto de test. Debe ser un valor\n","                             en el intervalo abierto (0, 1).\n","\n","        val_size [float] -- Tamaño del conjunto de validación. Debe ser un valor\n","                            en el intervalo abierto (0, 1).\n","\n","        text_column [str] -- Columna a utilizar como predictora (text/text_clean)\n","\n","\n","    Returns:\n","    -----\n","        train_df [pandas.DataFrame] -- Conjunto de datos de entrenamiento.\n","        val_df [pandas.DataFrame] -- Conjunto de datos de validación.\n","        cleaned_train_df [pandas.DataFrame] -- Conjunto de datos de entrenamiento pre-procesado.\n","        cleaned_val_df [pandas.DataFrame] -- Conjunto de datos de validación pre-procesado.\n","    \"\"\"\n","    if feature not in ['text', 'text_clean']:\n","      raise('Error! La variable utilizada para entrenar debe ser \"text\" o \"text_clean\"')\n","\n","    # Almacenar variables descriptivas y objetivo (X, y)\n","    X = nlp_df[feature]\n","    y = nlp_df['target']\n","\n","    # Train/test split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n","    X_train, X_val, y_train,  y_val  = train_test_split(X_train, y_train, test_size=val_size, stratify=y_train)\n","\n","    # Formar datasets de entrenamiento y validación\n","    train_df = pd.concat([X_train, y_train], axis=1)\n","    test_df  = pd.concat([X_test, y_test], axis=1)\n","    val_df   = pd.concat([X_val, y_val], axis=1)\n","\n","    # Estandarizar nombre de las variables de los dataframes\n","    renamed_columns = {feature:'text', 'target':'sentiment'}\n","    train_df.rename(renamed_columns, axis=1, inplace=True)\n","    test_df.rename(renamed_columns, axis=1, inplace=True)\n","    val_df.rename(renamed_columns, axis=1, inplace=True)\n","\n","    # Mostrar dimensionalidad de los dataset de entrenamiento y validación.\n","    print('Tweets in Train Dataset = {} ({:.2f}%)'.format(train_df.shape[0], 100*train_df.shape[0]/nlp_df.shape[0]))\n","    print('Tweets in Test Dataset = {} ({:.2f}%)'.format(test_df.shape[0], 100*test_df.shape[0]/nlp_df.shape[0]))\n","    print('Tweets in Validation Dataset = {} ({:.2f}%)'.format(val_df.shape[0], 100*val_df.shape[0]/nlp_df.shape[0]))\n","    print('\\n')\n","\n","    print('Target distribution on train set')\n","    print(train_df.sentiment.value_counts())\n","    print('\\n')\n","\n","    print('Target distribution on test set')\n","    print(test_df.sentiment.value_counts())\n","    print('\\n')\n","\n","    print('Target distribution on validation set')\n","    print(val_df.sentiment.value_counts())\n","    print('\\n')\n","\n","\n","    return train_df, test_df, val_df\n","\n","  def download_model(self, model_url):\n","    \"\"\"\n","    Descargar modelo pre-entrenado desde un repositorio público.\n","\n","    Args:\n","    -----\n","      model_url [str] -- Url para utilizar el modelo pre-entrenado.\n","\n","\n","    Returns:\n","      tf_model -- Modelo pre-entrenado que se utilizará para realizar el transfer learning.\n","    \"\"\"\n","    print(f'Descargando modelo desde \"{model_url}\"')\n","    try:\n","      tf_model = hub.KerasLayer(model_url, input_shape=[], dtype=tf.string, trainable=True)\n","      print(tf_model)\n","    except Exception as e:\n","      raise('Error! No se pudo descargar el modelo pre-entrenado!')\n","      return None\n","    else:\n","      print('Modelo pre-entrenado descargado con exito!')\n","      return tf_model\n","\n","  def define_model(self, model_url, model_name):\n","    \"\"\"\n","    Permite crear una red neuronal por medio de transfer learning.\n","\n","    Args:\n","    -----\n","      model_url [str] -- Url para utilizar el modelo pre-entrenado.\n","      model_name [str] -- Nombre del modelo pre-entrenado utilizado.\n","    \"\"\"\n","\n","    # Descargar modelo pre-entrenado\n","    # Construir modelo de análisis de sentimiento.\n","    print('Building model...')\n","    model = tf.keras.Sequential(name=model_name)\n","    hub_model = self.download_model(model_url)\n","    model.add(hub_model) \n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(128, activation='relu'))\n","    model.add(tf.keras.layers.Dense(10, activation='relu'))\n","    model.add(tf.keras.layers.Dropout(0.5))\n","    model.add(tf.keras.layers.Dense(1, activation='sigmoid', name='predictions'))\n","\n","    print(f'--{model_name} Model Summary --')\n","    print(model.summary())\n","\n","    # Compilar modelo\n","    model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['binary_accuracy'])\n","    \n","    # Guardar modelo en atributo de la clase.\n","    self.models[model_name] = model\n","    print(f'Total defined models = {len(self.models.keys())}')\n","    print(2*'\\n')\n","\n","  def train(self, model_name, train_df, val_df, test_df, epochs=5, batch_size=216, patience=20, drive_path=None):\n","    \"\"\"\n","    Permite entrenar una red neuronal por medio de transfer learning definida\n","    previametne.\n","    \n","    Nota: Tanto el conjunto de entrenamiento como el conjunto de validación debe \n","    ser un dataframe de pandas con las siguientes columnas:\n","      - text. Texto de los tweets.\n","      - sentiment. Sentimiento del tweet codificado como positivo (1) o negativo (0).\n","\n","\n","    Args:\n","    -----\n","      model_name [str] -- Nombre del modelo previamente compilado.\n","      train_df [pandas.DataFrame] -- Conjunto de entrenamiento.\n","      val_df [pandas.DataFrame] -- Conjunto de validación.\n","      test_df [pandas.DataFrame] -- Conjunto de test.\n","      epochs [int] -- Epócas de entrenamiento (Default: 20).\n","      batch_size [int] -- (Default: 216).\n","      patience=20 [float] -- Paciencia (Default = 20)\n","    \"\"\"\n","    if model_name not in self.models.keys():\n","      raise ('Error! El modelo indicado no ha sido compilado. Revise el nombre del modelo!')\n","\n","    print('Training...')\n","    print('Patience =', patience)\n","    print('Epochs =',epochs)\n","    print('Batch Size =', batch_size)\n","    m = self.models[model_name]\n","    print(m.summary())\n","\n","    # Callbacks\n","    logdir = os.path.join('logs', datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","    val_loss_callback = EarlyStopping(monitor='val_loss', patience=patience)\n","    val_accuracy_callback = EarlyStopping(monitor='val_binary_accuracy', patience=patience)\n","\n","    # Entrenamiento\n","    t = time.perf_counter()\n","    history = m.fit(train_df['text'], train_df['sentiment'],\n","                    epochs=epochs, \n","                    use_multiprocessing=True, \n","                    batch_size = batch_size,\n","                    validation_data = (val_df['text'], val_df['sentiment']),\n","                    callbacks = [\n","                                tensorboard_callback,\n","                                val_loss_callback,\n","                                val_accuracy_callback\n","                    ])\n","    elapsed_time = datetime.timedelta(seconds=(time.perf_counter()-t))\n","    print('Train elapsed time =', elapsed_time)\n","\n","    # Guarda el modelo.\n","    self.models[model_name] = m\n","    if drive_path is not None:\n","      ks.models.save_model(m,\n","                          filepath=drive_path + f'{model_name}.h5',\n","                          overwrite=True,\n","                          include_optimizer=True,\n","                          save_format='h5'\n","      )\n","      \n","    # Evaluar modelo\n","    filename_figure = drive_path + f'{model_name}.png'\n","    self.evaluate_model(model_name, history, test_df, filename_figure)\n","\n","  def evaluate_model(self, model_name, history, test_df, filename_figure):\n","    \"\"\"\n","    Evaluación del modelo.\n","\n","    Args:\n","      model_name [str] -- Nombre del modelo previamente compilado.\n","      history [] -- Historia de entrenamiento del modelo.\n","      test_df [pandas.DataFrame] -- Conjunto de test\n","    \"\"\"\n","    print(f'Evaluating {model_name}...')\n","    if model_name not in self.models.keys():\n","      raise ('Error! El modelo indicado no ha sido compilado. Revise el nombre del modelo!')\n","\n","    # Seleccionar modelo\n","    m = self.models[model_name]\n","\n","    # Evaluación del modelo\n","    _, acc = m.evaluate(test_df['text'], test_df['sentiment'])\n","\n","    print('Accuracy = {:.3f}%'.format(acc*100))\n","\n","    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(30, 25))\n","\n","    # Loss\n","    _ = sns.lineplot(data=history.history['loss'], color='blue', label='train', marker='o', ax=ax1)\n","    _ = sns.lineplot(data=history.history['val_loss'], color='orange', label='train', marker='o', ax=ax1)\n","    _ = ax1.set_xlabel('Epochs', fontsize=20)\n","    _ = ax1.set_ylabel('Loss', fontsize=20)\n","    _ = ax1.set_title('Cross Entropy Loss', fontsize=28)\n","    _ = ax1.tick_params(axis='x', labelsize=14)\n","    _ = ax1.tick_params(axis='y', labelsize=14)\n","\n","    # Accuracy\n","    _ = sns.lineplot(data=history.history['binary_accuracy'], color='blue', label='val', marker='o', ax=ax2)\n","    _ = sns.lineplot(data=history.history['val_binary_accuracy'], color='orange', label='train', marker='o', ax=ax2)\n","    _ = ax2.set_xlabel('Epochs', fontsize=20)\n","    _ = ax2.set_ylabel('Accuracy', fontsize=20)\n","    _ = ax2.set_title('Binary Classification Accuracy', fontsize=28)\n","    _ = ax2.tick_params(axis='x', labelsize=14)\n","    _ = ax2.tick_params(axis='y', labelsize=14)\n","\n","    _ = plt.show()\n","    fig.savefig(filename_figure)\n","\n","  def load_model_from_file(self, model_name):\n","    \"\"\"\n","    Lectura de modelo almacenado localmente en formato 'tf'\n","\n","    Args:\n","      model_name [str] -- Nombre del modelo\n","    \"\"\"\n","    m = ks.models.load_model(f'models/{model_name}.h5' ,custom_objects={'KerasLayer':hub.KerasLayer})\n","    m.summary()\n","    self.models[model_name] = m\n","    print(f'Total defined models = {len(self.models.keys())}')\n","    print(2*'\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7720,"status":"ok","timestamp":1648550022889,"user":{"displayName":"Sergio M","userId":"05689582408971891814"},"user_tz":-120},"id":"1HAWSPeP0DNv","outputId":"3c660b43-518d-4038-b07c-6572773620a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando dataset...\n","Dimensionalidad: (1600000, 6)\n"]}],"source":["# Cargar los datos\n","print('Cargando dataset...')\n","df = pd.read_csv(filename_path, encoding='latin', header = None)\n","df.columns = ['target', 'ids', 'date','flag','user','text']\n","df.loc[df['target']==4,'target']=1\n","print('Dimensionalidad:',df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH7Zi-aWrdL8"},"outputs":[],"source":["# Diccionario de nombre y sitio de descarga de los modelos pre-entrenados\n","tf_hub_models = {\n","   #'gnews_swivel_20dim_with_oov':'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1',\n","   #'gnews_swivel_20dim':'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1',\n","   'universal_sentence_encoder': 'https://tfhub.dev/google/universal-sentence-encoder/4',\n","   #'nnlm_en_dim50':'https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1',\n","   #'nnlm_en_dim128':'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1',\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57Avn7NoECTl","outputId":"7ec241cb-5d92-4da3-be2d-66555fd4b7d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["FaysPy Twitter Natural Language Processing created!\n","Dimensionalidad: (1600000, 6)\n","Selecting vars\n","Drop duplicates...\n","Dimensionalidad: (1583691, 2)\n","Cleaning ascii...\n","Lower text...\n","Cleaning stopwords...\n","Cleaning english punctuations...\n","Cleaning repeating chars...\n","Cleaning emails...\n","Cleaning urls...\n","Cleaning numbers...\n","Tokenizing text...\n","Applying stemming...\n","Applying lemmatizer...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Tweets in Train Dataset = 1140256 (72.00%)\n","Tweets in Test Dataset = 316739 (20.00%)\n","Tweets in Validation Dataset = 126696 (8.00%)\n","\n","\n","Target distribution on train set\n","1    571323\n","0    568933\n","Name: sentiment, dtype: int64\n","\n","\n","Target distribution on test set\n","1    158702\n","0    158037\n","Name: sentiment, dtype: int64\n","\n","\n","Target distribution on validation set\n","1    63481\n","0    63215\n","Name: sentiment, dtype: int64\n","\n","\n","Building model...\n","Descargando modelo desde \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n","<tensorflow_hub.keras_layer.KerasLayer object at 0x7fa5bbafbe90>\n","Modelo pre-entrenado descargado con exito!\n","--universal_sentence_encoder Model Summary --\n","Model: \"universal_sentence_encoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer (KerasLayer)    (None, 512)               256797824 \n","                                                                 \n"," flatten (Flatten)           (None, 512)               0         \n","                                                                 \n"," dense (Dense)               (None, 128)               65664     \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                1290      \n","                                                                 \n"," dropout (Dropout)           (None, 10)                0         \n","                                                                 \n"," predictions (Dense)         (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 256,864,789\n","Trainable params: 256,864,789\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Total defined models = 1\n","\n","\n","\n","Training...\n","Patience = 100\n","Epochs = 600\n","Batch Size = 512\n","Model: \"universal_sentence_encoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer (KerasLayer)    (None, 512)               256797824 \n","                                                                 \n"," flatten (Flatten)           (None, 512)               0         \n","                                                                 \n"," dense (Dense)               (None, 128)               65664     \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                1290      \n","                                                                 \n"," dropout (Dropout)           (None, 10)                0         \n","                                                                 \n"," predictions (Dense)         (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 256,864,789\n","Trainable params: 256,864,789\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/600\n","2228/2228 [==============================] - 633s 279ms/step - loss: 0.4950 - binary_accuracy: 0.7599 - val_loss: 0.4437 - val_binary_accuracy: 0.7917\n","Epoch 2/600\n","2228/2228 [==============================] - 620s 278ms/step - loss: 0.3634 - binary_accuracy: 0.8366 - val_loss: 0.5048 - val_binary_accuracy: 0.7802\n","Epoch 3/600\n","2228/2228 [==============================] - 622s 279ms/step - loss: 0.1804 - binary_accuracy: 0.9220 - val_loss: 0.7620 - val_binary_accuracy: 0.7661\n","Epoch 4/600\n","2228/2228 [==============================] - 621s 279ms/step - loss: 0.0901 - binary_accuracy: 0.9552 - val_loss: 1.1798 - val_binary_accuracy: 0.7575\n","Epoch 5/600\n","2228/2228 [==============================] - 622s 279ms/step - loss: 0.0665 - binary_accuracy: 0.9621 - val_loss: 1.6869 - val_binary_accuracy: 0.7508\n","Epoch 6/600\n","1153/2228 [==============>...............] - ETA: 4:33 - loss: 0.0575 - binary_accuracy: 0.9638"]}],"source":["%%time\n","# Instanciar objeto\n","fayspy = Fayspy_Twitter()\n","\n","# Preparar dataset\n","nlp_df = fayspy.prepare_dataset(df)\n","\n","# Train/Test split\n","train_df, test_df, val_df = fayspy.train_test_split(nlp_df, test_size=.2, val_size=.1, feature='text')\n","\n","# Definir modelos de redes neuronales\n","for model_name in tf_hub_models.keys():\n","  fayspy.define_model(tf_hub_models[model_name], model_name)\n","  fayspy.train(model_name=model_name, \n","               train_df=train_df, val_df=val_df, test_df=test_df, \n","               epochs=600, batch_size=512, patience=100, \n","               drive_path='/content/drive/MyDrive/Colab Notebooks/TweetSentimentAnalysis/models/')\n","  \n","  print(4*'\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Sentiment_Analysis_TransferLearning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}